<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression · DynamicHMCExamples.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DynamicHMCExamples.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Overview</a></li><li><a class="toctext" href="example_independent_bernoulli.html">Estimate Bernoulli draws probabilility</a></li><li class="current"><a class="toctext" href="example_linear_regression.html">Linear regression</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href="example_linear_regression.html">Linear regression</a></li></ul><a class="edit-page" href="https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_linear_regression.jl"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Linear regression</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Linear-regression-1" href="#Linear-regression-1">Linear regression</a></h1><p>We estimate simple linear regression model with a half-T prior. First, we load the packages we use.</p><div><pre><code class="language-julia">using TransformVariables, LogDensityProblems, DynamicHMC, MCMCDiagnostics,
    Parameters, Statistics, Distributions</code></pre></div><p>Then define a structure to hold the data: observables, covariates, and the degrees of freedom for the prior.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
Linear regression model ``y ∼ Xβ + ϵ``, where ``ϵ ∼ N(0, σ²)`` IID.

Flat prior for `β`, half-T for `σ`.
&quot;&quot;&quot;
struct LinearRegressionProblem{TY &lt;: AbstractVector, TX &lt;: AbstractMatrix,
                               Tν &lt;: Real}
    &quot;Observations.&quot;
    y::TY
    &quot;Covariates&quot;
    X::TX
    &quot;Degrees of freedom for prior.&quot;
    ν::Tν
end</code></pre><pre><code class="language-none">Main.ex-example_linear_regression.LinearRegressionProblem</code></pre></div><p>Then make the type callable with the parameters <em>as a single argument</em>.</p><div><pre><code class="language-julia">function (problem::LinearRegressionProblem)(θ)
    @unpack y, X, ν = problem   # extract the data
    @unpack β, σ = θ            # works on the named tuple too
    loglikelihood(Normal(0, σ), y .- X*β) + logpdf(TDist(ν), σ)
end</code></pre></div><p>We should test this, also, this would be a good place to benchmark and optimize more complicated problems.</p><div><pre><code class="language-julia">N = 100
X = hcat(ones(N), randn(N, 2));
β = [1.0, 2.0, -1.0]
σ = 0.5
y = X*β .+ randn(N) .* σ;
p = LinearRegressionProblem(y, X, 1.0);
p((β = β, σ = σ))</code></pre><pre><code class="language-none">-64.86685696877704</code></pre></div><p>For this problem, we write a function to return the transformation (as it varies with the number of covariates).</p><div><pre><code class="language-julia">problem_transformation(p::LinearRegressionProblem) =
    as((β = as(Array, size(p.X, 2)), σ = asℝ₊))</code></pre></div><p>Wrap the problem with a transformation, then use Flux for the gradient.</p><div><pre><code class="language-julia">P = TransformedLogDensity(problem_transformation(p), p)
∇P = FluxGradientLogDensity(P);</code></pre></div><p>Finally, we sample from the posterior. <code>chain</code> holds the chain (positions and diagnostic information), while the second returned value is the tuned sampler which would allow continuation of sampling.</p><div><pre><code class="language-julia">chain, NUTS_tuned = NUTS_init_tune_mcmc(∇P, 1000);</code></pre><pre><code class="language-none">MCMC, adapting ϵ (75 steps)
 ...done
MCMC, adapting ϵ (25 steps)
 ...done
MCMC, adapting ϵ (50 steps)
 ...done
MCMC, adapting ϵ (100 steps)
step 100/100, 11000.0 s/step
 ...done
MCMC, adapting ϵ (200 steps)
step 100/200, 11000.0 s/step
step 200/200, 8800.0 s/step
 ...done
MCMC, adapting ϵ (400 steps)
step 100/400, 12000.0 s/step
step 200/400, 9100.0 s/step
step 300/400, 9600.0 s/step
step 400/400, 8400.0 s/step
 ...done
MCMC, adapting ϵ (50 steps)
 ...done
MCMC (1000 steps)
step 100/1000, 11000.0 s/step
step 200/1000, 11000.0 s/step
step 300/1000, 11000.0 s/step
step 400/1000, 9900.0 s/step
step 500/1000, 10000.0 s/step
step 600/1000, 10000.0 s/step
step 700/1000, 10000.0 s/step
step 800/1000, 11000.0 s/step
step 900/1000, 11000.0 s/step
step 1000/1000, 11000.0 s/step
 ...done</code></pre></div><p>We use the transformation to obtain the posterior from the chain.</p><div><pre><code class="language-julia">posterior = transform.(Ref(∇P.transformation), get_position.(chain));</code></pre></div><p>Extract the parameter posterior means: <code>β</code>,</p><div><pre><code class="language-julia">posterior_β = mean(first, posterior)</code></pre><pre><code class="language-none">3-element Array{Float64,1}:
  0.9088177360258037
  1.955210995161812
 -1.0299559451702975</code></pre></div><p>then <code>σ</code>:</p><div><pre><code class="language-julia">posterior_σ = mean(last, posterior)</code></pre><pre><code class="language-none">0.4533087048761664</code></pre></div><p>Effective sample sizes (of untransformed draws)</p><div><pre><code class="language-julia">ess = mapslices(effective_sample_size,
                get_position_matrix(chain); dims = 1)</code></pre></div><p>NUTS-specific statistics</p><div><pre><code class="language-julia">NUTS_statistics(chain)</code></pre><pre><code class="language-none">Hamiltonian Monte Carlo sample of length 1000
  acceptance rate mean: 0.93, min/25%/median/75%/max: 0.44 0.9 0.96 0.99 1.0
  termination: AdjacentTurn =&gt; 15% DoubledTurn =&gt; 85%
  depth: 1 =&gt; 1% 2 =&gt; 42% 3 =&gt; 57%</code></pre></div><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><footer><hr/><a class="previous" href="example_independent_bernoulli.html"><span class="direction">Previous</span><span class="title">Estimate Bernoulli draws probabilility</span></a></footer></article></body></html>
