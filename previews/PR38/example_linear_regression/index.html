<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression · DynamicHMCExamples.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">DynamicHMCExamples.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../example_independent_bernoulli/">Estimate Bernoulli draws probabilility</a></li><li class="is-active"><a class="tocitem" href>Linear regression</a></li><li><a class="tocitem" href="../example_logistic_regression/">Logistic regression</a></li><li><a class="tocitem" href="../example_multinomial_logistic_regression/">Multinomial logistic regression</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Linear regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_linear_regression.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h1><p>We estimate simple linear regression model with a half-T prior. First, we load the packages we use.</p><p>First, we import DynamicHMC and related libraries,</p><pre><code class="language- hljs">using TransformVariables, LogDensityProblems, DynamicHMC, TransformedLogDensities</code></pre><p>then some packages that help code the log posterior,</p><pre><code class="language-julia hljs">using Parameters, Statistics, Random, Distributions, LinearAlgebra</code></pre><p>then diagnostic and benchmark tools,</p><pre><code class="language- hljs">using MCMCDiagnosticTools, DynamicHMC.Diagnostics, BenchmarkTools</code></pre><p>and use ForwardDiff for AD since the dimensions is small.</p><pre><code class="language-julia hljs">import ForwardDiff</code></pre><p>Then define a structure to hold the data: observables, covariates, and the degrees of freedom for the prior.</p><pre><code class="language-julia hljs">&quot;&quot;&quot;
Linear regression model ``y ∼ Xβ + ϵ``, where ``ϵ ∼ N(0, σ²)`` IID.

Weakly informative prior for `β`, half-T for `σ`.
&quot;&quot;&quot;
struct LinearRegressionProblem{TY &lt;: AbstractVector, TX &lt;: AbstractMatrix,
                               Tν &lt;: Real}
    &quot;Observations.&quot;
    y::TY
    &quot;Covariates&quot;
    X::TX
    &quot;Degrees of freedom for prior.&quot;
    ν::Tν
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.LinearRegressionProblem</code></pre><p>Then make the type callable with the parameters <em>as a single argument</em>.</p><pre><code class="language-julia hljs">function (problem::LinearRegressionProblem)(θ)
    @unpack y, X, ν = problem   # extract the data
    @unpack β, σ = θ            # works on the named tuple too
    ϵ_distribution = Normal(0, σ) # the error term
    ℓ_error = mapreduce((y, x) -&gt; logpdf(ϵ_distribution, y - dot(x, β)), +,
                        y, eachrow(X))    # likelihood for error
    ℓ_σ = logpdf(TDist(ν), σ)             # prior for σ
    ℓ_β = loglikelihood(Normal(0, 10), β) # prior for β
    ℓ_error + ℓ_σ + ℓ_β
end</code></pre><p>Make up random data and test the function runs.</p><pre><code class="language-julia hljs">N = 100
X = hcat(ones(N), randn(N, 2));
β = [1.0, 2.0, -1.0]
σ = 0.5
y = X*β .+ randn(N) .* σ;
p = LinearRegressionProblem(y, X, 1.0);
p((β = β, σ = σ))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-77.64899754242104</code></pre><p>It is usually a good idea to benchmark and optimize your log posterior code at this stage. Above, we have carefully optimized allocations away using <code>mapreduce</code>.</p><pre><code class="language- hljs">@btime p((β = $β, σ = $σ))</code></pre><p>For this problem, we write a function to return the transformation (as it varies with the number of covariates).</p><pre><code class="language-julia hljs">function problem_transformation(p::LinearRegressionProblem)
    as((β = as(Array, size(p.X, 2)), σ = asℝ₊))
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">problem_transformation (generic function with 1 method)</code></pre><p>Wrap the problem with a transformation, then use ForwardDiff for the gradient.</p><pre><code class="language- hljs">t = problem_transformation(p)
P = TransformedLogDensity(t, p)
∇P = ADgradient(:ForwardDiff, P);
nothing #hide</code></pre><p>Finally, we sample from the posterior. <code>chain</code> holds the chain (positions and diagnostic information), while the second returned value is the tuned sampler which would allow continuation of sampling.</p><pre><code class="language- hljs">results = map(_ -&gt; mcmc_with_warmup(Random.default_rng(), ∇P, 1000), 1:5)</code></pre><p>We use the transformation to obtain the posterior from the chain.</p><pre><code class="language- hljs">posterior = transform.(t, eachcol(pool_posterior_matrices(results)));
nothing #hide</code></pre><p>Extract the parameter posterior means: <code>β</code>,</p><pre><code class="language- hljs">posterior_β = mean(first, posterior)</code></pre><p>then <code>σ</code>:</p><pre><code class="language- hljs">posterior_σ = mean(last, posterior)</code></pre><p>Effective sample sizes (of untransformed draws)</p><pre><code class="language- hljs">ess, R̂ = ess_rhat(stack_posterior_matrices(results))</code></pre><p>summarize NUTS-specific statistics of all chains</p><pre><code class="language- hljs">summarize_tree_statistics(mapreduce(x -&gt; x.tree_statistics, vcat, results))</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example_independent_bernoulli/">« Estimate Bernoulli draws probabilility</a><a class="docs-footer-nextpage" href="../example_logistic_regression/">Logistic regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 2 September 2022 10:18">Friday 2 September 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
